# LiteLLM config to proxy Ollama local models on macOS

model_list:
  - model_name: ollama/*
    litellm_params:
      api_base: ${LITELLM_OLLAMA_API_BASE}
      model_info:
        base_model: ollama

general_settings: 
  master_key: ${LITELLM_MASTER_KEY}
  # We can generate additional API keys via the admin interface
  service_tokens: 
    - "Bearer ${LITELLM_API_KEY}"

router_settings:
  num_retries: 3
  timeout: 120

general_settings:
  debug: true                                         # Enable debug logs for troubleshooting
  telemetry: true                                     # Disable telemetry if privacy desired